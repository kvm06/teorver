import numpy as np
import seaborn as sns

# 1. Находим коэффициенты используя intercept
# Представим модель в виде y = a + bx

X = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
y = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])
b = (np.mean(X * y) - np.mean(X) * np.mean(y))/(np.mean(X**2)-np.mean(X)**2) #2.620538882402765
a = np.mean(y) - b * np.mean(X) #444.1773573243596
print(b)
print(a)
y_hat = 444.17 + 2.62 * X
print(y_hat)
#[535.87 562.07 941.97 968.17 548.97 627.57 585.65 837.17 758.57 732.37]

#Посчитаем без использования intercept, y = B * X, с помощью матричного метода поиска коэффициентов
X = X.reshape((10, 1))
print(X)
y = y.reshape((10, 1))
print(y)

B = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T@y)
print(B) #[[5.88982042]]

#2. Посчитаем коэффициент линейной регрессии используя градиентный спуск

X = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
y = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])
n = 10
#Формула нахождения среднестатистической ошибки
def mse_(B1, y=y, X=X, n=10):
    return np.sum((B1 * X - y)**2) / n
#обозначим alpha скорость обучения
alpha = 1e-6
B1 = 0.1

for i in range(100):
    B1 -= alpha*(2/n)*np.sum((B1 * X - y) * X)
    print(f'B1 = {B1}, mse={mse_(B1)}')
#B1 = 0.25952808, mse=493237.7212546963
#B1 = 0.414660650906144, mse=469503.15593253804
#B1 = 0.5655188230595969, mse=447058.4982813111
#...

#3. Таблица критических точек распределения стьюдента используется для проверки статистической гипотезы
# о том, что истинное значение случайной величины находится в пределах доверительного интервала,
# в случае, если неизвестна сигма - стандартное отклонение генеральной совокупности.
# функция Лапласа - это вероятность того, что случайная величина примет значение,
# принадлежащее заданному интервалу. Таким образом, данные критерии используют для построения разных гипотез. 
